iimport random, numpy, math, gym

from keras.models import Sequential
from keras.layers import *
from keras.optimizers import *

class Agent():
    def __init__(self, env, state_count, action_count):
        
        self.env = env
        
        self.stateCnt = state_count
        self.actionCnt = action_count
    
        self.model = self.buildModel()
        
        self.samples = []
        
        self.capacity = 100000
        self.batch_size = 64
        self.gamma = 0.99
        self.max_epsilon = 1
        self.min_epsilon = 0.01
        self.epsilon = 1
        self.decay = 0.001
        
    def buildModel(self):
            model = Sequential()
                
            model.add(Dense(24, activation = 'relu', input_dim = self.stateCnt))
            model.add(Dense(24, activation = 'relu'))
            model.add(Dense(self.actionCnt, activation = 'linear'))
                
            model.compile(loss = 'mse', optimizer = Adam(lr = 0.001))
            
            return model
    
    
    def act(self, s):
        if random.random() < self.epsilon:
            return random.randint(0, self.actionCnt - 1)
        else:
            return numpy.argmax(self.predict(s))
    
    
    def add(self, state, action, reward, next_state, done):
            self.samples.append((state, action, reward, next_state, done))
    
    
    def replay(self, batch_size):
        minibatch = random.sample(self.samples, batch_size)
        
        for state, action, reward, next_state, done in minibatch:
            target = reward
            
            if not done:
              target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])
            
            target_f = self.model.predict(state)
            target_f[0][action] = target
            self.model.fit(state, target_f, epochs=1, verbose=0)
        
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.decay
    
    
    def run(self, episodes, test = False):
        
        for i in range(episodes):
            s = self.env.reset()
            s = np.reshape(s, [1, 4])

            
            if test == True:
                self.env.render()
            
            while True:
                a = self.act(s)
                s_, r, done, info = self.env.step(a)
                
                self.add(s, a, r, s_, done)
                
                s = s_
                
                if done:
                    
                    if test == True:
                        print('Episode ' + i + " reward: " + r)
                    
                    break
            
            self.replay(32)
                

env = gym.make('CartPole-v0')
stateCnt  = env.observation_space.shape[0]
actionCnt = env.action_space.n

a = Agent(env, stateCnt, actionCnt)

a.run(10000)

a.run(10, test = True)




