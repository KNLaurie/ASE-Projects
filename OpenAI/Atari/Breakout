import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.autograd import Variable
import torchvision.transforms as T

import gym
import math
import numpy as np
import random

from gym import wrappers
from itertools import count
from collections import namedtuple
import csv

#--------------------IMAGE PREPROCESSING-------------------------

class Frame_pro():
    def img_transform(self, screen):
        screen = T.ToPILImage()(screen)
        screen = T.Resize((84, 84))(screen)
        return screen
    
    def process(self, screen):
        screen = screen[34:-16,:]
        screen = self.img_transform(screen)
        screen = T.Grayscale()(screen)
        screen = T.ToTensor()(screen)
        return screen.squeeze(1)
        


#-------------------NETWORK--------------------------------------

class NN(nn.Module):
    def __init__(self, in_frames = 4, n_actions = 3):
        super(NN,self).__init__()        
        
        self.conv1= nn.Conv2d(in_frames,32,8,4) 
        self.conv2= nn.Conv2d(32,64,4,2)
        self.conv3= nn.Conv2d(64,64,3,1)

        self.fc1= nn.Linear(64*7*7,1500)
        self.fc2= nn.Linear(1500,n_actions)
        
        
    def forward(self, x):
        h= F.relu(self.conv1(x))
        h= F.relu(self.conv2(h))
        h= F.relu(self.conv3(h))
        h=h.view(x.size(0), -1)
        h= F.relu(self.fc1(h))
        h= self.fc2(h)
        return h



#------------------------MEMORY----------------------------------

class Memory(object):
    
    def __init__(self, size):
        super(Memory, self).__init__()
        self.memory = []
        self.current_size = 0
        self.capacity = size
        
    def push(self, *args):
        Transition = namedtuple("Transition", ["state", "action", "reward", "next_state", "done"])
        self.memory.append(Transition(*args))
        self.current_size += 1
        
        if self.current_size > self.capacity:
            self.memory.pop(0)
            self.current_size -= 1

    def sample(self, batch_size = 32):
        return random.sample(self.memory, batch_size)
    
    def __len__(self): 
        return self.current_size


#---------------------AGENT-------------------------------------

class Agent():
    def __init__(self, action_count = 4, state_count = 210, batch_size = 32, max_memory = 10000, episodes = 10000,
                 discount = 0.99, epsilon = 0.1, max_epsilon = 0.1, min_epsilon = 0.01, decay = 0.95, decay_steps = 50000):
        
        self.actCnt = action_count
        self.stateCnt = state_count
        self.batchSize = batch_size
        self.capacity = max_memory
        self.num_episodes = episodes
        self.Transition = namedtuple("Transition", ["state", "action", "reward", "next_state", "done"])
        
        self.DISCOUNT = discount
        self.EPSILON = epsilon
        self.MAX_EPSILON = epsilon
        self.MIN_EPSILON = min_epsilon
        self.DECAY = decay
        
        self.steps = 0
        self.decay_steps = decay_steps
        
        self.replay_memory = Memory(self.capacity)
        self.image = Frame_pro()
        
        self.q_net = NN(n_actions = self.actCnt)
        self.target_net = NN(n_actions = self.actCnt)
        
    
    def make_policy(self, network, nA):
        
        def policy(observation, epsilon):
            A = np.ones(nA, dtype=float) * epsilon / nA
            q_values = network(observation)
            _, best_action = q_values.max(1)
            best_action= best_action.data.numpy()
            A[best_action] += (1.0 - epsilon)
            return A
        
        return policy
 
    
    def optimize(self, batch_size, replay_memory, q_predict, target_estimator, optimizer, discount):
        
        batch = self.Transition(*zip(*replay_memory.sample(batch_size)))
        
        non_final_mask = torch.ByteTensor(tuple(not i for i in batch.done))
        non_final_next_states = Variable(torch.cat([s for s in batch.next_state if s is not None]))
            
        state_batch = Variable(torch.cat(batch.state))
        action_batch = Variable(torch.LongTensor(batch.action))
        reward_batch = Variable(torch.Tensor(batch.reward))
        state_action_values = Variable(q_predict(state_batch).gather(1, action_batch.unsqueeze(1)), requires_grad = False)

        next_state_values = Variable(torch.zeros(batch_size))
        
        next_state_values = target_estimator(non_final_next_states).max(1)[0]

        targets = ((next_state_values * self.DISCOUNT) + reward_batch)
    
        loss = F.smooth_l1_loss(targets.view(batch_size, 1), state_action_values)
            
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    def action(self, state, actions):
        self.steps += 1
        
        self.EPSILON = self.MIN_EPSILON + (self.MAX_EPSILON - self.MIN_EPSILON) * math.exp(-self.DECAY * self.steps)
        
        if random.random() < self.EPSILON:
            return random.choice(actions)           
        
        else:
           with torch.no_grad():
            return self.q_net(state).max(1)[1].view(1,1)
    
    def copy_params(self, estimator1, estimator2):
        
        estimator2.load_state_dict(estimator1.state_dict())
    
    def save_checkpoint(self, state, filename):
        torch.save(state, filename)
        print(f"=> saving checkpoint '{filename}' at episode {state['epoch']}")
    
   
    
##-------------------------------MAIN-------------------------------------------------    
    
    def main(self):
        rewards = []
        start_episode = 0
        total_t = 0
        skip = True
        optimizer= torch.optim.RMSprop(self.q_net.parameters(),lr=0.00025, momentum=0, weight_decay=0.99, eps=1e-6)
        VALID_ACTIONS = [0, 1, 2, 3]
        model_name = "Breakout"
        
        q_values=[]
        episode_lengths=np.zeros(10000)
        episode_rewards=np.zeros(10000)

        self.target_net.load_state_dict(self.q_net.state_dict())

        env = gym.envs.make("Breakout-v0")
        
       
        state = env.reset()
        state = self.image.process(state) 
        state = torch.cat([state.clone() for i in range(4)]) 
        state = state.unsqueeze(0) 
        
        self.copy_params(self.q_net, self.target_net)
            
        print("Populating replay memory")
        
        for i in range(10000):
            action = env.action_space.sample()
            new_state, reward, done, _= env.step(action)
            new_state = self.image.process(new_state).unsqueeze(0)
            new_state = torch.cat([state[:,1:,:,:], new_state],dim=1)
            self.replay_memory.push(state,action,reward,new_state,done)
            

            state = new_state

            if done:
                
                state= env.reset()
                state= self.image.process(state) 
                state= torch.cat([state.clone() for i in range(4)])
                state=state.unsqueeze(0)
            
            if i % 1000 == 0:
                print("stored memory: " + str(i))
        
        print("Training")
        env = wrappers.Monitor(env, '/u/lauriek/PythonExercise/OpenAi/Atari', video_callable = False, force = True)
                              
        for ep in range(0, self.num_episodes):
            state = env.reset()
            state = self.image.process(state) 
            state = torch.cat([state.clone() for i in range(4)])
            state = state.unsqueeze(0)
            
            k = 0
            action=None
            loss = None
            done = False
            
            if ep > 0 and ep % 50 == 0:
                self.save_checkpoint({'epoch': ep + 1,
                                 'total_t':total_t,
                                 'state_dict': self.q_net.state_dict(),
                                 'optimizer' : optimizer.state_dict(),
                                 'episode_lengths':episode_lengths,
                                 'episode_rewards':episode_rewards,
                                }, model_name+'_chkpt.pth')
            
                
            while not done:
                if action is not None:
                    for _ in range(4):
                        if not done:
                            _,_,done,_= env.step(action)
                   
                
                if total_t % 100:
                    self.copy_params(self.q_net, self.target_net)
               
                if not done:
                    if total_t != 0:
                        action = self.action(state, VALID_ACTIONS)
                            
                    else:
                        action = 1
                        
                    new_state,reward,done,_= env.step(action)
                    
                    new_state = self.image.process(new_state).unsqueeze(0)
                    new_state = torch.cat([state[:,1:,:,:], new_state],dim=1)

                       
                  
                    self.replay_memory.push(state, action, reward, new_state, done)

                    if k % 50 == 0: 
                        loss = self.optimize(self.batchSize, self.replay_memory, self.q_net,
                                         self.target_net, optimizer, self.DISCOUNT)

                    state = new_state
                total_t += 1
                k += 1
                episode_rewards[ep] += reward
                episode_lengths[ep] += 1
                print(k)
        
        with open('EpisodeRewards.txt', 'w') as f:
            f.write('Rewards\n')
            f.write(episode_rewards)
        
        
        for _ in range(0, 10):
            
            
            for ep in range(0, 10):
                state = env.reset()
                
                state = self.image.process(state) 
                state = torch.cat([state.clone() for i in range(4)])
                state = state.unsqueeze(0)

                k = 4
                action=None
                loss = None
                done = False
            
                    
                while not done:
                    if action is not None:
                        for _ in range(4):
                            if not done:
                                _,_,done,_= env.step(action)
                       
                    
                        
                   
                    if not done:
                        if total_t != 0:
                            action = self.action(state, VALID_ACTIONS)
                                
                        else:
                            action = 1
                            
                        new_state,reward,done,_= env.step(action)
                        
                        new_state = self.image.process(new_state).unsqueeze(0)
                        new_state = torch.cat([state[:,1:,:,:], new_state],dim=1)

                    state = new_state
                    total_t += 1          
    
    
    
#----------------------------RUN-------------------------------
A = Agent()

A.main()
         

